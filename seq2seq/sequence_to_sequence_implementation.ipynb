{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Character Sequence to Sequence \n",
    "In this notebook, we'll build a model that takes in a sequence of letters, and outputs a sorted version of that sequence. We'll do that using what we've learned so far about Sequence to Sequence models. This notebook was updated to work with TensorFlow 1.1 and builds on the work of Dave Currie. Check out Dave's post [Text Summarization with Amazon Reviews](https://medium.com/towards-data-science/text-summarization-with-amazon-reviews-41801c2210b).\n",
    "\n",
    "<img src=\"images/sequence-to-sequence.jpg\"/>\n",
    "\n",
    "\n",
    "## Dataset \n",
    "\n",
    "The dataset lives in the /data/ folder. At the moment, it is made up of the following files:\n",
    " * **letters_source.txt**: The list of input letter sequences. Each sequence is its own line. \n",
    " * **letters_target.txt**: The list of target sequences we'll use in the training process. Each sequence here is a response to the input sequence in letters_source.txt with the same line number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import helper\n",
    "\n",
    "source_path = 'data/letters_source.txt'\n",
    "target_path = 'data/letters_target.txt'\n",
    "\n",
    "source_sentences = helper.load_data(source_path)\n",
    "target_sentences = helper.load_data(target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by examining the current state of the dataset. `source_sentences` contains the entire input sequence file as text delimited by newline symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bsaqq',\n",
       " 'npy',\n",
       " 'lbwuj',\n",
       " 'bqv',\n",
       " 'kial',\n",
       " 'tddam',\n",
       " 'edxpjpg',\n",
       " 'nspv',\n",
       " 'huloz',\n",
       " '']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_sentences[:50].split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`target_sentences` contains the entire output sequence file as text delimited by newline symbols.  Each line corresponds to the line from `source_sentences`.  `target_sentences` contains a sorted characters of the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abqqs',\n",
       " 'npy',\n",
       " 'bjluw',\n",
       " 'bqv',\n",
       " 'aikl',\n",
       " 'addmt',\n",
       " 'degjppx',\n",
       " 'npsv',\n",
       " 'hlouz',\n",
       " '']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_sentences[:50].split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "To do anything useful with it, we'll need to turn the each string into a list of characters: \n",
    "\n",
    "<img src=\"images/source_and_target_arrays.png\"/>\n",
    "\n",
    "Then convert the characters to their int values as declared in our vocabulary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example source sequence\n",
      "[[13, 11, 20, 6, 6], [10, 17, 4], [8, 13, 22, 5, 12]]\n",
      "\n",
      "\n",
      "Example target sequence\n",
      "[[20, 13, 6, 6, 11, 3], [10, 17, 4, 3], [13, 12, 8, 5, 22, 3]]\n"
     ]
    }
   ],
   "source": [
    "def extract_character_vocab(data):\n",
    "    special_words = ['<PAD>', '<UNK>', '<GO>',  '<EOS>']\n",
    "\n",
    "    set_words = set([character for line in data.split('\\n') for character in line])\n",
    "    int_to_vocab = {word_i: word for word_i, word in enumerate(special_words + list(set_words))}\n",
    "    vocab_to_int = {word: word_i for word_i, word in int_to_vocab.items()}\n",
    "\n",
    "    return int_to_vocab, vocab_to_int\n",
    "\n",
    "# Build int2letter and letter2int dicts\n",
    "source_int_to_letter, source_letter_to_int = extract_character_vocab(source_sentences)\n",
    "target_int_to_letter, target_letter_to_int = extract_character_vocab(target_sentences)\n",
    "\n",
    "# Convert characters to ids\n",
    "source_letter_ids = [[source_letter_to_int.get(letter, source_letter_to_int['<UNK>']) for letter in line] for line in source_sentences.split('\\n')]\n",
    "target_letter_ids = [[target_letter_to_int.get(letter, target_letter_to_int['<UNK>']) for letter in line] + [target_letter_to_int['<EOS>']] for line in target_sentences.split('\\n')] \n",
    "\n",
    "print(\"Example source sequence\")\n",
    "print(source_letter_ids[:3])\n",
    "print(\"\\n\")\n",
    "print(\"Example target sequence\")\n",
    "print(target_letter_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This is the final shape we need them to be in. We can now proceed to building the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "#### Check the Version of TensorFlow\n",
    "This will check to make sure you have the correct version of TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "epochs = 60\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# RNN Size\n",
    "rnn_size = 50\n",
    "# Number of Layers\n",
    "num_layers = 2\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 15\n",
    "decoding_embedding_size = 15\n",
    "# Learning Rate\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_inputs():\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "    target_sequence_length = tf.placeholder(tf.int32, (None,), name='target_sequence_length')\n",
    "    max_target_sequence_length = tf.reduce_max(target_sequence_length, name='max_target_len')\n",
    "    source_sequence_length = tf.placeholder(tf.int32, (None,), name='source_sequence_length')\n",
    "    \n",
    "    return input_data, targets, lr, target_sequence_length, max_target_sequence_length, source_sequence_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence to Sequence Model\n",
    "\n",
    "We can now start defining the functions that will build the seq2seq model. We are building it from the bottom up with the following components:\n",
    "\n",
    "    2.1 Encoder\n",
    "        - Embedding\n",
    "        - Encoder cell\n",
    "    2.2 Decoder\n",
    "        1- Process decoder inputs\n",
    "        2- Set up the decoder\n",
    "            - Embedding\n",
    "            - Decoder cell\n",
    "            - Dense output layer\n",
    "            - Training decoder\n",
    "            - Inference decoder\n",
    "    2.3 Seq2seq model connecting the encoder and decoder\n",
    "    2.4 Build the training graph hooking up the model with the \n",
    "        optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Encoder\n",
    "\n",
    "The first bit of the model we'll build is the encoder. Here, we'll embed the input data, construct our encoder, then pass the embedded data to the encoder.\n",
    "\n",
    "- Embed the input data using [`tf.contrib.layers.embed_sequence`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embed_sequence)\n",
    "<img src=\"images/embed_sequence.png\" />\n",
    "\n",
    "- Pass the embedded input into a stack of RNNs.  Save the RNN state and ignore the output.\n",
    "<img src=\"images/encoder.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def encoding_layer(input_data, rnn_size, num_layers,\n",
    "                   source_sequence_length, source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "\n",
    "\n",
    "    # Encoder embedding\n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(input_data, source_vocab_size, encoding_embedding_size)\n",
    "\n",
    "    # RNN cell\n",
    "    def make_cell(rnn_size):\n",
    "        enc_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        return enc_cell\n",
    "\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    enc_output, enc_state = tf.nn.dynamic_rnn(enc_cell, enc_embed_input, sequence_length=source_sequence_length, dtype=tf.float32)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Decoder\n",
    "\n",
    "The decoder is probably the most involved part of this model. The following steps are needed to create it:\n",
    "\n",
    "    1- Process decoder inputs\n",
    "    2- Set up the decoder components\n",
    "        - Embedding\n",
    "        - Decoder cell\n",
    "        - Dense output layer\n",
    "        - Training decoder\n",
    "        - Inference decoder\n",
    "\n",
    "\n",
    "### Process Decoder Input\n",
    "\n",
    "\n",
    "In the training process, the target sequences will be used in two different places:\n",
    "\n",
    " 1. Using them to calculate the loss\n",
    " 2. Feeding them to the decoder during training to make the model more robust.\n",
    "\n",
    "Now we need to address the second point. Let's assume our targets look like this in their letter/word form (we're doing this for readibility. At this point in the code, these sequences would be in int form):\n",
    "\n",
    "\n",
    "<img src=\"images/targets_1.png\"/>\n",
    "\n",
    "We need to do a simple transformation on the tensor before feeding it to the decoder:\n",
    "\n",
    "1- We will feed an item of the sequence to the decoder at each time step. Think about the last timestep -- where the decoder outputs the final word in its output. The input to that step is the item before last from the target sequence. The decoder has no use for the last item in the target sequence in this scenario. So we'll need to remove the last item. \n",
    "\n",
    "We do that using tensorflow's tf.strided_slice() method. We hand it the tensor, and the index of where to start and where to end the cutting.\n",
    "\n",
    "<img src=\"images/strided_slice_1.png\"/>\n",
    "\n",
    "2- The first item in each sequence we feed to the decoder has to be GO symbol. So We'll add that to the beginning.\n",
    "\n",
    "\n",
    "<img src=\"images/targets_add_go.png\"/>\n",
    "\n",
    "\n",
    "Now the tensor is ready to be fed to the decoder. It looks like this (if we convert from ints to letters/symbols):\n",
    "\n",
    "<img src=\"images/targets_after_processing_1.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the input we'll feed to the decoder\n",
    "def process_decoder_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Set up the decoder components\n",
    "\n",
    "        - Embedding\n",
    "        - Decoder cell\n",
    "        - Dense output layer\n",
    "        - Training decoder\n",
    "        - Inference decoder\n",
    "\n",
    "#### 1- Embedding\n",
    "Now that we have prepared the inputs to the training decoder, we need to embed them so they can be ready to be passed to the decoder. \n",
    "\n",
    "We'll create an embedding matrix like the following then have tf.nn.embedding_lookup convert our input to its embedded equivalent:\n",
    "<img src=\"images/embeddings.png\" />\n",
    "\n",
    "#### 2- Decoder Cell\n",
    "Then we declare our decoder cell. Just like the encoder, we'll use an tf.contrib.rnn.LSTMCell here as well.\n",
    "\n",
    "We need to declare a decoder for the training process, and a decoder for the inference/prediction process. These two decoders will share their parameters (so that all the weights and biases that are set during the training phase can be used when we deploy the model).\n",
    "\n",
    "First, we'll need to define the type of cell we'll be using for our decoder RNNs. We opted for LSTM.\n",
    "\n",
    "#### 3- Dense output layer\n",
    "Before we move to declaring our decoders, we'll need to create the output layer, which will be a tensorflow.python.layers.core.Dense layer that translates the outputs of the decoder to logits that tell us which element of the decoder vocabulary the decoder is choosing to output at each time step.\n",
    "\n",
    "#### 4- Training decoder\n",
    "Essentially, we'll be creating two decoders which share their parameters. One for training and one for inference. The two are similar in that both created using tf.contrib.seq2seq.**BasicDecoder** and tf.contrib.seq2seq.**dynamic_decode**. They differ, however, in that we feed the the target sequences as inputs to the training decoder at each time step to make it more robust.\n",
    "\n",
    "We can think of the training decoder as looking like this (except that it works with sequences in batches):\n",
    "<img src=\"images/sequence-to-sequence-training-decoder.png\"/>\n",
    "\n",
    "The training decoder **does not** feed the output of each time step to the next. Rather, the inputs to the decoder time steps are the target sequence from the training dataset (the orange letters).\n",
    "\n",
    "#### 5- Inference decoder\n",
    "The inference decoder is the one we'll use when we deploy our model to the wild.\n",
    "\n",
    "<img src=\"images/sequence-to-sequence-inference-decoder.png\"/>\n",
    "\n",
    "We'll hand our encoder hidden state to both the training and inference decoders and have it process its output. TensorFlow handles most of the logic for us. We just have to use the appropriate methods from tf.contrib.seq2seq and supply them with the appropriate inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(target_letter_to_int, decoding_embedding_size, num_layers, rnn_size,\n",
    "                   target_sequence_length, max_target_sequence_length, enc_state, dec_input):\n",
    "    # 1. Decoder Embedding\n",
    "    target_vocab_size = len(target_letter_to_int)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "\n",
    "    # 2. Construct the decoder cell\n",
    "    def make_cell(rnn_size):\n",
    "        dec_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        return dec_cell\n",
    "\n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "     \n",
    "    # 3. Dense layer to translate the decoder's output at each time \n",
    "    # step into a choice from the target vocabulary\n",
    "    output_layer = Dense(target_vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "\n",
    "\n",
    "    # 4. Set up a training decoder and an inference decoder\n",
    "    # Training Decoder\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "\n",
    "        # Helper for the training process. Used by BasicDecoder to read inputs.\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                            sequence_length=target_sequence_length,\n",
    "                                                            time_major=False)\n",
    "        \n",
    "        \n",
    "        # Basic decoder\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                           training_helper,\n",
    "                                                           enc_state,\n",
    "                                                           output_layer) \n",
    "        \n",
    "        # Perform dynamic decoding using the decoder\n",
    "        training_decoder_output = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                                       impute_finished=True,\n",
    "                                                                       maximum_iterations=max_target_sequence_length)[0]\n",
    "    # 5. Inference Decoder\n",
    "    # Reuses the same parameters trained by the training process\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        start_tokens = tf.tile(tf.constant([target_letter_to_int['<GO>']], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "\n",
    "        # Helper for the inference process.\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                target_letter_to_int['<EOS>'])\n",
    "\n",
    "        # Basic decoder\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        enc_state,\n",
    "                                                        output_layer)\n",
    "        \n",
    "        # Perform dynamic decoding using the decoder\n",
    "        inference_decoder_output = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_target_sequence_length)[0]\n",
    "         \n",
    "\n",
    "    \n",
    "    return training_decoder_output, inference_decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Seq2seq model \n",
    "Let's now go a step above, and hook up the encoder and decoder using the methods we just declared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seq2seq_model(input_data, targets, lr, target_sequence_length, \n",
    "                  max_target_sequence_length, source_sequence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size, \n",
    "                  rnn_size, num_layers):\n",
    "    \n",
    "    # Pass the input data through the encoder. We'll ignore the encoder output, but use the state\n",
    "    _, enc_state = encoding_layer(input_data, \n",
    "                                  rnn_size, \n",
    "                                  num_layers, \n",
    "                                  source_sequence_length,\n",
    "                                  source_vocab_size, \n",
    "                                  encoding_embedding_size)\n",
    "    \n",
    "    \n",
    "    # Prepare the target sequences we'll feed to the decoder in training mode\n",
    "    dec_input = process_decoder_input(targets, target_letter_to_int, batch_size)\n",
    "    \n",
    "    # Pass encoder state and decoder inputs to the decoders\n",
    "    training_decoder_output, inference_decoder_output = decoding_layer(target_letter_to_int, \n",
    "                                                                       decoding_embedding_size, \n",
    "                                                                       num_layers, \n",
    "                                                                       rnn_size,\n",
    "                                                                       target_sequence_length,\n",
    "                                                                       max_target_sequence_length,\n",
    "                                                                       enc_state, \n",
    "                                                                       dec_input) \n",
    "    \n",
    "    return training_decoder_output, inference_decoder_output\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model outputs *training_decoder_output* and *inference_decoder_output* both contain a 'rnn_output' logits tensor that looks like this:\n",
    "\n",
    "<img src=\"images/logits.png\"/>\n",
    "\n",
    "The logits we get from the training tensor we'll pass to tf.contrib.seq2seq.**sequence_loss()** to calculate the loss and ultimately the gradient.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, target_sequence_length, max_target_sequence_length, source_sequence_length = get_model_inputs()\n",
    "    \n",
    "    # Create the training and inference logits\n",
    "    training_decoder_output, inference_decoder_output = seq2seq_model(input_data, \n",
    "                                                                      targets, \n",
    "                                                                      lr, \n",
    "                                                                      target_sequence_length, \n",
    "                                                                      max_target_sequence_length, \n",
    "                                                                      source_sequence_length,\n",
    "                                                                      len(source_letter_to_int),\n",
    "                                                                      len(target_letter_to_int),\n",
    "                                                                      encoding_embedding_size, \n",
    "                                                                      decoding_embedding_size, \n",
    "                                                                      rnn_size, \n",
    "                                                                      num_layers)    \n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_decoder_output.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_decoder_output.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        \n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Batches\n",
    "\n",
    "There's little processing involved when we retreive the batches. This is a simple example assuming batch_size = 2\n",
    "\n",
    "Source sequences (it's actually in int form, we're showing the characters for clarity):\n",
    "\n",
    "<img src=\"images/source_batch.png\" />\n",
    "\n",
    "Target sequences (also in int, but showing letters for clarity):\n",
    "\n",
    "<img src=\"images/target_batch.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(targets, sources, batch_size, source_pad_int, target_pad_int):\n",
    "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "        \n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "        \n",
    "        yield pad_targets_batch, pad_sources_batch, pad_targets_lengths, pad_source_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "We're now ready to train our model. If you run into OOM (out of memory) issues during training, try to decrease the batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/60 Batch   20/77 - Loss:  2.514  - Validation loss:  2.512\n",
      "Epoch   1/60 Batch   40/77 - Loss:  2.297  - Validation loss:  2.250\n",
      "Epoch   1/60 Batch   60/77 - Loss:  1.980  - Validation loss:  2.028\n",
      "Epoch   2/60 Batch   20/77 - Loss:  1.673  - Validation loss:  1.757\n",
      "Epoch   2/60 Batch   40/77 - Loss:  1.700  - Validation loss:  1.655\n",
      "Epoch   2/60 Batch   60/77 - Loss:  1.522  - Validation loss:  1.559\n",
      "Epoch   3/60 Batch   20/77 - Loss:  1.388  - Validation loss:  1.455\n",
      "Epoch   3/60 Batch   40/77 - Loss:  1.470  - Validation loss:  1.428\n",
      "Epoch   3/60 Batch   60/77 - Loss:  1.371  - Validation loss:  1.401\n",
      "Epoch   4/60 Batch   20/77 - Loss:  1.257  - Validation loss:  1.324\n",
      "Epoch   4/60 Batch   40/77 - Loss:  1.318  - Validation loss:  1.283\n",
      "Epoch   4/60 Batch   60/77 - Loss:  1.190  - Validation loss:  1.233\n",
      "Epoch   5/60 Batch   20/77 - Loss:  1.118  - Validation loss:  1.166\n",
      "Epoch   5/60 Batch   40/77 - Loss:  1.170  - Validation loss:  1.137\n",
      "Epoch   5/60 Batch   60/77 - Loss:  1.045  - Validation loss:  1.100\n",
      "Epoch   6/60 Batch   20/77 - Loss:  0.998  - Validation loss:  1.048\n",
      "Epoch   6/60 Batch   40/77 - Loss:  1.041  - Validation loss:  1.030\n",
      "Epoch   6/60 Batch   60/77 - Loss:  0.945  - Validation loss:  1.002\n",
      "Epoch   7/60 Batch   20/77 - Loss:  0.901  - Validation loss:  0.965\n",
      "Epoch   7/60 Batch   40/77 - Loss:  0.953  - Validation loss:  0.953\n",
      "Epoch   7/60 Batch   60/77 - Loss:  0.876  - Validation loss:  0.928\n",
      "Epoch   8/60 Batch   20/77 - Loss:  0.814  - Validation loss:  0.887\n",
      "Epoch   8/60 Batch   40/77 - Loss:  0.856  - Validation loss:  0.859\n",
      "Epoch   8/60 Batch   60/77 - Loss:  0.767  - Validation loss:  0.814\n",
      "Epoch   9/60 Batch   20/77 - Loss:  0.690  - Validation loss:  0.758\n",
      "Epoch   9/60 Batch   40/77 - Loss:  0.736  - Validation loss:  0.734\n",
      "Epoch   9/60 Batch   60/77 - Loss:  0.658  - Validation loss:  0.703\n",
      "Epoch  10/60 Batch   20/77 - Loss:  0.597  - Validation loss:  0.664\n",
      "Epoch  10/60 Batch   40/77 - Loss:  0.653  - Validation loss:  0.642\n",
      "Epoch  10/60 Batch   60/77 - Loss:  0.574  - Validation loss:  0.620\n",
      "Epoch  11/60 Batch   20/77 - Loss:  0.530  - Validation loss:  0.582\n",
      "Epoch  11/60 Batch   40/77 - Loss:  0.586  - Validation loss:  0.566\n",
      "Epoch  11/60 Batch   60/77 - Loss:  0.514  - Validation loss:  0.550\n",
      "Epoch  12/60 Batch   20/77 - Loss:  0.477  - Validation loss:  0.520\n",
      "Epoch  12/60 Batch   40/77 - Loss:  0.528  - Validation loss:  0.506\n",
      "Epoch  12/60 Batch   60/77 - Loss:  0.474  - Validation loss:  0.489\n",
      "Epoch  13/60 Batch   20/77 - Loss:  0.432  - Validation loss:  0.471\n",
      "Epoch  13/60 Batch   40/77 - Loss:  0.477  - Validation loss:  0.453\n",
      "Epoch  13/60 Batch   60/77 - Loss:  0.410  - Validation loss:  0.439\n",
      "Epoch  14/60 Batch   20/77 - Loss:  0.381  - Validation loss:  0.417\n",
      "Epoch  14/60 Batch   40/77 - Loss:  0.432  - Validation loss:  0.406\n",
      "Epoch  14/60 Batch   60/77 - Loss:  0.370  - Validation loss:  0.398\n",
      "Epoch  15/60 Batch   20/77 - Loss:  0.345  - Validation loss:  0.377\n",
      "Epoch  15/60 Batch   40/77 - Loss:  0.396  - Validation loss:  0.368\n",
      "Epoch  15/60 Batch   60/77 - Loss:  0.335  - Validation loss:  0.360\n",
      "Epoch  16/60 Batch   20/77 - Loss:  0.313  - Validation loss:  0.341\n",
      "Epoch  16/60 Batch   40/77 - Loss:  0.365  - Validation loss:  0.335\n",
      "Epoch  16/60 Batch   60/77 - Loss:  0.306  - Validation loss:  0.330\n",
      "Epoch  17/60 Batch   20/77 - Loss:  0.286  - Validation loss:  0.312\n",
      "Epoch  17/60 Batch   40/77 - Loss:  0.336  - Validation loss:  0.308\n",
      "Epoch  17/60 Batch   60/77 - Loss:  0.282  - Validation loss:  0.305\n",
      "Epoch  18/60 Batch   20/77 - Loss:  0.262  - Validation loss:  0.289\n",
      "Epoch  18/60 Batch   40/77 - Loss:  0.311  - Validation loss:  0.285\n",
      "Epoch  18/60 Batch   60/77 - Loss:  0.260  - Validation loss:  0.282\n",
      "Epoch  19/60 Batch   20/77 - Loss:  0.240  - Validation loss:  0.267\n",
      "Epoch  19/60 Batch   40/77 - Loss:  0.288  - Validation loss:  0.264\n",
      "Epoch  19/60 Batch   60/77 - Loss:  0.238  - Validation loss:  0.259\n",
      "Epoch  20/60 Batch   20/77 - Loss:  0.220  - Validation loss:  0.246\n",
      "Epoch  20/60 Batch   40/77 - Loss:  0.265  - Validation loss:  0.244\n",
      "Epoch  20/60 Batch   60/77 - Loss:  0.217  - Validation loss:  0.239\n",
      "Epoch  21/60 Batch   20/77 - Loss:  0.200  - Validation loss:  0.227\n",
      "Epoch  21/60 Batch   40/77 - Loss:  0.244  - Validation loss:  0.226\n",
      "Epoch  21/60 Batch   60/77 - Loss:  0.196  - Validation loss:  0.219\n",
      "Epoch  22/60 Batch   20/77 - Loss:  0.181  - Validation loss:  0.207\n",
      "Epoch  22/60 Batch   40/77 - Loss:  0.222  - Validation loss:  0.207\n",
      "Epoch  22/60 Batch   60/77 - Loss:  0.175  - Validation loss:  0.200\n",
      "Epoch  23/60 Batch   20/77 - Loss:  0.165  - Validation loss:  0.188\n",
      "Epoch  23/60 Batch   40/77 - Loss:  0.202  - Validation loss:  0.190\n",
      "Epoch  23/60 Batch   60/77 - Loss:  0.159  - Validation loss:  0.185\n",
      "Epoch  24/60 Batch   20/77 - Loss:  0.149  - Validation loss:  0.181\n",
      "Epoch  24/60 Batch   40/77 - Loss:  0.185  - Validation loss:  0.182\n",
      "Epoch  24/60 Batch   60/77 - Loss:  0.141  - Validation loss:  0.169\n",
      "Epoch  25/60 Batch   20/77 - Loss:  0.132  - Validation loss:  0.155\n",
      "Epoch  25/60 Batch   40/77 - Loss:  0.170  - Validation loss:  0.154\n",
      "Epoch  25/60 Batch   60/77 - Loss:  0.126  - Validation loss:  0.151\n",
      "Epoch  26/60 Batch   20/77 - Loss:  0.117  - Validation loss:  0.140\n",
      "Epoch  26/60 Batch   40/77 - Loss:  0.149  - Validation loss:  0.139\n",
      "Epoch  26/60 Batch   60/77 - Loss:  0.111  - Validation loss:  0.136\n",
      "Epoch  27/60 Batch   20/77 - Loss:  0.105  - Validation loss:  0.126\n",
      "Epoch  27/60 Batch   40/77 - Loss:  0.135  - Validation loss:  0.126\n",
      "Epoch  27/60 Batch   60/77 - Loss:  0.099  - Validation loss:  0.124\n",
      "Epoch  28/60 Batch   20/77 - Loss:  0.094  - Validation loss:  0.116\n",
      "Epoch  28/60 Batch   40/77 - Loss:  0.123  - Validation loss:  0.116\n",
      "Epoch  28/60 Batch   60/77 - Loss:  0.089  - Validation loss:  0.114\n",
      "Epoch  29/60 Batch   20/77 - Loss:  0.086  - Validation loss:  0.109\n",
      "Epoch  29/60 Batch   40/77 - Loss:  0.112  - Validation loss:  0.115\n",
      "Epoch  29/60 Batch   60/77 - Loss:  0.084  - Validation loss:  0.109\n",
      "Epoch  30/60 Batch   20/77 - Loss:  0.103  - Validation loss:  0.153\n",
      "Epoch  30/60 Batch   40/77 - Loss:  0.108  - Validation loss:  0.131\n",
      "Epoch  30/60 Batch   60/77 - Loss:  0.082  - Validation loss:  0.103\n",
      "Epoch  31/60 Batch   20/77 - Loss:  0.071  - Validation loss:  0.098\n",
      "Epoch  31/60 Batch   40/77 - Loss:  0.089  - Validation loss:  0.092\n",
      "Epoch  31/60 Batch   60/77 - Loss:  0.068  - Validation loss:  0.090\n",
      "Epoch  32/60 Batch   20/77 - Loss:  0.065  - Validation loss:  0.090\n",
      "Epoch  32/60 Batch   40/77 - Loss:  0.081  - Validation loss:  0.085\n",
      "Epoch  32/60 Batch   60/77 - Loss:  0.061  - Validation loss:  0.084\n",
      "Epoch  33/60 Batch   20/77 - Loss:  0.059  - Validation loss:  0.085\n",
      "Epoch  33/60 Batch   40/77 - Loss:  0.074  - Validation loss:  0.079\n",
      "Epoch  33/60 Batch   60/77 - Loss:  0.056  - Validation loss:  0.078\n",
      "Epoch  34/60 Batch   20/77 - Loss:  0.055  - Validation loss:  0.081\n",
      "Epoch  34/60 Batch   40/77 - Loss:  0.068  - Validation loss:  0.074\n",
      "Epoch  34/60 Batch   60/77 - Loss:  0.051  - Validation loss:  0.073\n",
      "Epoch  35/60 Batch   20/77 - Loss:  0.050  - Validation loss:  0.077\n",
      "Epoch  35/60 Batch   40/77 - Loss:  0.062  - Validation loss:  0.070\n",
      "Epoch  35/60 Batch   60/77 - Loss:  0.046  - Validation loss:  0.069\n",
      "Epoch  36/60 Batch   20/77 - Loss:  0.047  - Validation loss:  0.073\n",
      "Epoch  36/60 Batch   40/77 - Loss:  0.058  - Validation loss:  0.066\n",
      "Epoch  36/60 Batch   60/77 - Loss:  0.042  - Validation loss:  0.065\n",
      "Epoch  37/60 Batch   20/77 - Loss:  0.043  - Validation loss:  0.070\n",
      "Epoch  37/60 Batch   40/77 - Loss:  0.053  - Validation loss:  0.062\n",
      "Epoch  37/60 Batch   60/77 - Loss:  0.038  - Validation loss:  0.061\n",
      "Epoch  38/60 Batch   20/77 - Loss:  0.040  - Validation loss:  0.067\n",
      "Epoch  38/60 Batch   40/77 - Loss:  0.049  - Validation loss:  0.059\n",
      "Epoch  38/60 Batch   60/77 - Loss:  0.035  - Validation loss:  0.058\n",
      "Epoch  39/60 Batch   20/77 - Loss:  0.037  - Validation loss:  0.064\n",
      "Epoch  39/60 Batch   40/77 - Loss:  0.046  - Validation loss:  0.056\n",
      "Epoch  39/60 Batch   60/77 - Loss:  0.032  - Validation loss:  0.055\n",
      "Epoch  40/60 Batch   20/77 - Loss:  0.034  - Validation loss:  0.062\n",
      "Epoch  40/60 Batch   40/77 - Loss:  0.042  - Validation loss:  0.053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  40/60 Batch   60/77 - Loss:  0.030  - Validation loss:  0.052\n",
      "Epoch  41/60 Batch   20/77 - Loss:  0.032  - Validation loss:  0.059\n",
      "Epoch  41/60 Batch   40/77 - Loss:  0.039  - Validation loss:  0.050\n",
      "Epoch  41/60 Batch   60/77 - Loss:  0.027  - Validation loss:  0.050\n",
      "Epoch  42/60 Batch   20/77 - Loss:  0.029  - Validation loss:  0.057\n",
      "Epoch  42/60 Batch   40/77 - Loss:  0.037  - Validation loss:  0.048\n",
      "Epoch  42/60 Batch   60/77 - Loss:  0.025  - Validation loss:  0.048\n",
      "Epoch  43/60 Batch   20/77 - Loss:  0.027  - Validation loss:  0.055\n",
      "Epoch  43/60 Batch   40/77 - Loss:  0.034  - Validation loss:  0.046\n",
      "Epoch  43/60 Batch   60/77 - Loss:  0.023  - Validation loss:  0.046\n",
      "Epoch  44/60 Batch   20/77 - Loss:  0.025  - Validation loss:  0.052\n",
      "Epoch  44/60 Batch   40/77 - Loss:  0.032  - Validation loss:  0.044\n",
      "Epoch  44/60 Batch   60/77 - Loss:  0.022  - Validation loss:  0.044\n",
      "Epoch  45/60 Batch   20/77 - Loss:  0.023  - Validation loss:  0.050\n",
      "Epoch  45/60 Batch   40/77 - Loss:  0.030  - Validation loss:  0.042\n",
      "Epoch  45/60 Batch   60/77 - Loss:  0.020  - Validation loss:  0.043\n",
      "Epoch  46/60 Batch   20/77 - Loss:  0.022  - Validation loss:  0.049\n",
      "Epoch  46/60 Batch   40/77 - Loss:  0.028  - Validation loss:  0.041\n",
      "Epoch  46/60 Batch   60/77 - Loss:  0.019  - Validation loss:  0.042\n",
      "Epoch  47/60 Batch   20/77 - Loss:  0.020  - Validation loss:  0.047\n",
      "Epoch  47/60 Batch   40/77 - Loss:  0.026  - Validation loss:  0.040\n",
      "Epoch  47/60 Batch   60/77 - Loss:  0.018  - Validation loss:  0.041\n",
      "Epoch  48/60 Batch   20/77 - Loss:  0.019  - Validation loss:  0.046\n",
      "Epoch  48/60 Batch   40/77 - Loss:  0.024  - Validation loss:  0.039\n",
      "Epoch  48/60 Batch   60/77 - Loss:  0.017  - Validation loss:  0.040\n",
      "Epoch  49/60 Batch   20/77 - Loss:  0.018  - Validation loss:  0.044\n",
      "Epoch  49/60 Batch   40/77 - Loss:  0.022  - Validation loss:  0.038\n",
      "Epoch  49/60 Batch   60/77 - Loss:  0.016  - Validation loss:  0.039\n",
      "Epoch  50/60 Batch   20/77 - Loss:  0.016  - Validation loss:  0.043\n",
      "Epoch  50/60 Batch   40/77 - Loss:  0.021  - Validation loss:  0.037\n",
      "Epoch  50/60 Batch   60/77 - Loss:  0.015  - Validation loss:  0.038\n",
      "Epoch  51/60 Batch   20/77 - Loss:  0.015  - Validation loss:  0.041\n",
      "Epoch  51/60 Batch   40/77 - Loss:  0.019  - Validation loss:  0.036\n",
      "Epoch  51/60 Batch   60/77 - Loss:  0.013  - Validation loss:  0.036\n",
      "Epoch  52/60 Batch   20/77 - Loss:  0.014  - Validation loss:  0.039\n",
      "Epoch  52/60 Batch   40/77 - Loss:  0.018  - Validation loss:  0.035\n",
      "Epoch  52/60 Batch   60/77 - Loss:  0.013  - Validation loss:  0.034\n",
      "Epoch  53/60 Batch   20/77 - Loss:  0.031  - Validation loss:  0.046\n",
      "Epoch  53/60 Batch   40/77 - Loss:  0.029  - Validation loss:  0.081\n",
      "Epoch  53/60 Batch   60/77 - Loss:  0.099  - Validation loss:  0.120\n",
      "Epoch  54/60 Batch   20/77 - Loss:  0.026  - Validation loss:  0.051\n",
      "Epoch  54/60 Batch   40/77 - Loss:  0.024  - Validation loss:  0.037\n",
      "Epoch  54/60 Batch   60/77 - Loss:  0.016  - Validation loss:  0.033\n",
      "Epoch  55/60 Batch   20/77 - Loss:  0.015  - Validation loss:  0.037\n",
      "Epoch  55/60 Batch   40/77 - Loss:  0.018  - Validation loss:  0.032\n",
      "Epoch  55/60 Batch   60/77 - Loss:  0.012  - Validation loss:  0.029\n",
      "Epoch  56/60 Batch   20/77 - Loss:  0.013  - Validation loss:  0.034\n",
      "Epoch  56/60 Batch   40/77 - Loss:  0.016  - Validation loss:  0.030\n",
      "Epoch  56/60 Batch   60/77 - Loss:  0.011  - Validation loss:  0.028\n",
      "Epoch  57/60 Batch   20/77 - Loss:  0.012  - Validation loss:  0.032\n",
      "Epoch  57/60 Batch   40/77 - Loss:  0.015  - Validation loss:  0.029\n",
      "Epoch  57/60 Batch   60/77 - Loss:  0.010  - Validation loss:  0.027\n",
      "Epoch  58/60 Batch   20/77 - Loss:  0.011  - Validation loss:  0.031\n",
      "Epoch  58/60 Batch   40/77 - Loss:  0.014  - Validation loss:  0.029\n",
      "Epoch  58/60 Batch   60/77 - Loss:  0.009  - Validation loss:  0.026\n",
      "Epoch  59/60 Batch   20/77 - Loss:  0.010  - Validation loss:  0.031\n",
      "Epoch  59/60 Batch   40/77 - Loss:  0.013  - Validation loss:  0.029\n",
      "Epoch  59/60 Batch   60/77 - Loss:  0.009  - Validation loss:  0.026\n",
      "Epoch  60/60 Batch   20/77 - Loss:  0.009  - Validation loss:  0.030\n",
      "Epoch  60/60 Batch   40/77 - Loss:  0.012  - Validation loss:  0.028\n",
      "Epoch  60/60 Batch   60/77 - Loss:  0.008  - Validation loss:  0.025\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Parent directory of best_model.ckpt doesn't exist, can't save.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: ; No such file or directory\n\t [[{{node save/SaveV2}} = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, EmbedSequence/embeddings, EmbedSequence/embeddings/Adam, EmbedSequence/embeddings/Adam_1, Variable, Variable/Adam, Variable/Adam_1, decode/decoder/dense/bias, decode/decoder/dense/bias/Adam, decode/decoder/dense/bias/Adam_1, decode/decoder/dense/kernel, decode/decoder/dense/kernel/Adam, decode/decoder/dense/kernel/Adam_1, decode/decoder/multi_rnn_cell/cell_0/lstm_cell/bias, decode/decoder/multi_rnn_cell/cell_0/lstm_cell/bias/Adam, decode/decoder/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1, decode/decoder/multi_rnn_cell/cell_0/lstm_cell/kernel, decode/decoder/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam, decode/decoder/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1, decode/decoder/multi_rnn_cell/cell_1/lstm_cell/bias, decode/decoder/multi_rnn_cell/cell_1/lstm_cell/bias/Adam, decode/decoder/multi_rnn_cell/cell_1/lstm_cell/bias/Adam_1, decode/decoder/multi_rnn_cell/cell_1/lstm_cell/kernel, decode/decoder/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam, decode/decoder/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam_1, optimization/beta1_power, optimization/beta2_power, rnn/multi_rnn_cell/cell_0/lstm_cell/bias, rnn/multi_rnn_cell/cell_0/lstm_cell/bias/Adam, rnn/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1, rnn/multi_rnn_cell/cell_0/lstm_cell/kernel, rnn/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam, rnn/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1, rnn/multi_rnn_cell/cell_1/lstm_cell/bias, rnn/multi_rnn_cell/cell_1/lstm_cell/bias/Adam, rnn/multi_rnn_cell/cell_1/lstm_cell/bias/Adam_1, rnn/multi_rnn_cell/cell_1/lstm_cell/kernel, rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam, rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1440\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: ; No such file or directory\n\t [[node save/SaveV2 (defined at <ipython-input-15-5691f7dc94e7>:54)  = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, EmbedSequence/embeddings, EmbedSequence/embeddings/Adam, EmbedSequence/embeddings/Adam_1, Variable, Variable/Adam, Variable/Adam_1, decode/decoder/dense/bias, decode/decoder/dense/bias/Adam, decode/decoder/dense/bias/Adam_1, decode/decoder/dense/kernel, decode/decoder/dense/kernel/Adam, decode/decoder/dense/kernel/Adam_1, decode/decoder/multi_rnn_cell/cell_0/lstm_cell/bias, decode/decoder/multi_rnn_cell/cell_0/lstm_cell/bias/Adam, decode/decoder/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1, decode/decoder/multi_rnn_cell/cell_0/lstm_cell/kernel, decode/decoder/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam, decode/decoder/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1, decode/decoder/multi_rnn_cell/cell_1/lstm_cell/bias, decode/decoder/multi_rnn_cell/cell_1/lstm_cell/bias/Adam, decode/decoder/multi_rnn_cell/cell_1/lstm_cell/bias/Adam_1, decode/decoder/multi_rnn_cell/cell_1/lstm_cell/kernel, decode/decoder/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam, decode/decoder/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam_1, optimization/beta1_power, optimization/beta2_power, rnn/multi_rnn_cell/cell_0/lstm_cell/bias, rnn/multi_rnn_cell/cell_0/lstm_cell/bias/Adam, rnn/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1, rnn/multi_rnn_cell/cell_0/lstm_cell/kernel, rnn/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam, rnn/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1, rnn/multi_rnn_cell/cell_1/lstm_cell/bias, rnn/multi_rnn_cell/cell_1/lstm_cell/bias/Adam, rnn/multi_rnn_cell/cell_1/lstm_cell/bias/Adam_1, rnn/multi_rnn_cell/cell_1/lstm_cell/kernel, rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam, rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam_1)]]\n\nCaused by op 'save/SaveV2', defined at:\n  File \"/home/mv/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/mv/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/mv/anaconda3/lib/python3.6/asyncio/base_events.py\", line 427, in run_forever\n    self._run_once()\n  File \"/home/mv/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1440, in _run_once\n    handle._run()\n  File \"/home/mv/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-5691f7dc94e7>\", line 54, in <module>\n    saver = tf.train.Saver()\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1102, in __init__\n    self.build()\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1114, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1151, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 792, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 284, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 202, in save_op\n    tensors)\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1690, in save_v2\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/home/mv/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nNotFoundError (see above for traceback): ; No such file or directory\n\t [[node save/SaveV2 (defined at <ipython-input-15-5691f7dc94e7>:54)  = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, EmbedSequence/embeddings, EmbedSequence/embeddings/Adam, EmbedSequence/embeddings/Adam_1, Variable, Variable/Adam, Variable/Adam_1, decode/decoder/dense/bias, decode/decoder/dense/bias/Adam, decode/decoder/dense/bias/Adam_1, decode/decoder/dense/kernel, decode/decoder/dense/kernel/Adam, decode/decoder/dense/kernel/Adam_1, decode/decoder/multi_rnn_cell/cell_0/lstm_cell/bias, decode/decoder/multi_rnn_cell/cell_0/lstm_cell/bias/Adam, decode/decoder/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1, decode/decoder/multi_rnn_cell/cell_0/lstm_cell/kernel, decode/decoder/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam, decode/decoder/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1, decode/decoder/multi_rnn_cell/cell_1/lstm_cell/bias, decode/decoder/multi_rnn_cell/cell_1/lstm_cell/bias/Adam, decode/decoder/multi_rnn_cell/cell_1/lstm_cell/bias/Adam_1, decode/decoder/multi_rnn_cell/cell_1/lstm_cell/kernel, decode/decoder/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam, decode/decoder/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam_1, optimization/beta1_power, optimization/beta2_power, rnn/multi_rnn_cell/cell_0/lstm_cell/bias, rnn/multi_rnn_cell/cell_0/lstm_cell/bias/Adam, rnn/multi_rnn_cell/cell_0/lstm_cell/bias/Adam_1, rnn/multi_rnn_cell/cell_0/lstm_cell/kernel, rnn/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam, rnn/multi_rnn_cell/cell_0/lstm_cell/kernel/Adam_1, rnn/multi_rnn_cell/cell_1/lstm_cell/bias, rnn/multi_rnn_cell/cell_1/lstm_cell/bias/Adam, rnn/multi_rnn_cell/cell_1/lstm_cell/bias/Adam_1, rnn/multi_rnn_cell/cell_1/lstm_cell/kernel, rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam, rnn/multi_rnn_cell/cell_1/lstm_cell/kernel/Adam_1)]]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5691f7dc94e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Save Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model Trained and Saved'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m               \"Parent directory of {} doesn't exist, can't save.\".format(\n\u001b[1;32m   1457\u001b[0m                   save_path))\n\u001b[0;32m-> 1458\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrite_meta_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Parent directory of best_model.ckpt doesn't exist, can't save."
     ]
    }
   ],
   "source": [
    "# Split data to training and validation sets\n",
    "train_source = source_letter_ids[batch_size:]\n",
    "train_target = target_letter_ids[batch_size:]\n",
    "valid_source = source_letter_ids[:batch_size]\n",
    "valid_target = target_letter_ids[:batch_size]\n",
    "(valid_targets_batch, valid_sources_batch, valid_targets_lengths, valid_sources_lengths) = next(get_batches(valid_target, valid_source, batch_size,\n",
    "                           source_letter_to_int['<PAD>'],\n",
    "                           target_letter_to_int['<PAD>']))\n",
    "\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "\n",
    "checkpoint = \"best_model.ckpt\" \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        for batch_i, (targets_batch, sources_batch, targets_lengths, sources_lengths) in enumerate(\n",
    "                get_batches(train_target, train_source, batch_size,\n",
    "                           source_letter_to_int['<PAD>'],\n",
    "                           target_letter_to_int['<PAD>'])):\n",
    "            \n",
    "            # Training step\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: sources_batch,\n",
    "                 targets: targets_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 source_sequence_length: sources_lengths})\n",
    "\n",
    "            # Debug message updating us on the status of the training\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                \n",
    "                # Calculate validation cost\n",
    "                validation_loss = sess.run(\n",
    "                [cost],\n",
    "                {input_data: valid_sources_batch,\n",
    "                 targets: valid_targets_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: valid_targets_lengths,\n",
    "                 source_sequence_length: valid_sources_lengths})\n",
    "                \n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}  - Validation loss: {:>6.3f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(train_source) // batch_size, \n",
    "                              loss, \n",
    "                              validation_loss[0]))\n",
    "\n",
    "    \n",
    "    \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, checkpoint)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    sequence_length = 7\n",
    "    return [source_letter_to_int.get(word, source_letter_to_int['<UNK>']) for word in text]+ [source_letter_to_int['<PAD>']]*(sequence_length-len(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File ./best_model.ckpt.meta does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5c04db457a57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloaded_graph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Load saved model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.meta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1672\u001b[0m   \"\"\"  # pylint: disable=g-doc-exception\n\u001b[1;32m   1673\u001b[0m   return _import_meta_graph_with_return_elements(\n\u001b[0;32m-> 1674\u001b[0;31m       meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\n\u001b[0m\u001b[1;32m   1675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_import_meta_graph_with_return_elements\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)\u001b[0m\n\u001b[1;32m   1684\u001b[0m                        \"execution is enabled.\")\n\u001b[1;32m   1685\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1686\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_meta_graph_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1687\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mread_meta_graph_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    631\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File %s does not exist.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m   \u001b[0;31m# First try to read it as a binary file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m   \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: File ./best_model.ckpt.meta does not exist."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_sentence = 'hello'\n",
    "text = source_to_seq(input_sentence)\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\"\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_length:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    \n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      target_sequence_length: [len(text)]*batch_size, \n",
    "                                      source_sequence_length: [len(text)]*batch_size})[0] \n",
    "\n",
    "\n",
    "pad = source_letter_to_int[\"<PAD>\"] \n",
    "\n",
    "print('Original Text:', input_sentence)\n",
    "\n",
    "print('\\nSource')\n",
    "print('  Word Ids:    {}'.format([i for i in text]))\n",
    "print('  Input Words: {}'.format(\" \".join([source_int_to_letter[i] for i in text])))\n",
    "\n",
    "print('\\nTarget')\n",
    "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "print('  Response Words: {}'.format(\" \".join([target_int_to_letter[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
